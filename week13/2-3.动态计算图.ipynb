{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600327505133",
   "display_name": "Python 3.7.0 64-bit ('pytorch1.6': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2-3 动态计算图\n",
    "本节我们将介绍Pytorch的动态计算图。\n",
    "包括：\n",
    "* 动态计算图简介\n",
    "* 计算图中的Function\n",
    "* 计算图和反向传播\n",
    "* 叶子节点和非叶子节点\n",
    "* 计算图在TensorBoard中的可视化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 一、动态计算图简介\n",
    "![](./data/torch动态图.gif)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。\n",
    "\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义。\n",
    "\n",
    "* 第一重含义是：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。\n",
    "* 第二重含义是：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放储存空间，下次调用需要重新创建。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.计算图的正向传播是立即执行的"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(12.0878)\ntensor([[ 1.3707],\n        [ 3.6980],\n        [ 5.1801],\n        [ 1.0187],\n        [ 3.3648],\n        [ 3.5799],\n        [ 2.2841],\n        [-1.1161],\n        [ 4.2858],\n        [ 1.5342]])\n"
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor([[3.0, 1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 1)\n",
    "y_hat = x@w.t() + b\n",
    "\n",
    "loss = torch.mean(torch.pow(y_hat-y, 2))\n",
    "print(loss.detach())\n",
    "print(y_hat.detach())"
   ]
  },
  {
   "source": [
    "### 2.计算图在反向传播后立即销毁"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1571cc4e791e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# backward调用之后, 立即销毁计算图\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 如果需要保留的话, 需要设置retain_graph = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loss.backward()  # 再次执行就会报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# backward调用之后, 立即销毁计算图\n",
    "loss.backward()  # 如果需要保留的话, 需要设置retain_graph = True\n",
    "\n",
    "# loss.backward()  # 再次执行就会报错"
   ]
  },
  {
   "source": [
    "## 二、计算图中的Function\n",
    "计算图中一般有着两种重要的节点, 一种是Tensor另一种就是Function, 实际上就是Pytorch中各种对张量操作的函数。\n",
    "\n",
    "这些Function和Python函数有一个重要的区别就是, 这里的Function同时包含了正向计算和反向传播的逻辑。\n",
    "\n",
    "我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 这里的grad_out就是upstream gradient\n",
    "        # 所以为了得到downstream gradient就只需要在这个函数里面计算local gradient\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[4.5000, 4.5000]])\ntensor([[4.5000]])\n"
    }
   ],
   "source": [
    "w = torch.tensor([[3., 1.]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "x = torch.tensor([[-1., -1.], [1., 1.]])\n",
    "y = torch.tensor([[2., 3]])\n",
    "\n",
    "myrelu = MyReLU.apply  # 函数句柄\n",
    "y_hat = myrelu(x@w.t() + b)\n",
    "loss = torch.mean(torch.pow(y_hat-y, 2))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<torch.autograd.function.MyReLUBackward object at 0x7f8c293d9908>\n"
    }
   ],
   "source": [
    "print(y_hat.grad_fn)"
   ]
  },
  {
   "source": [
    "## 三、计算图与反向传播\n",
    "了解了Function的功能, 我们可以简单地理解一下反向传播的原理和过程。理解该部分原理需要一些高等数学的基础知识。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y2_grad: tensor(4.)\ny1_grad: tensor(-4.)\nx_grad: tensor(4.)\n"
    }
   ],
   "source": [
    "from functools import partial\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "# 注册钩子函数看的更清楚\n",
    "def f(name, grad):\n",
    "    print(name+\"_grad:\", grad)\n",
    "y1_f = partial(f, \"y1\")\n",
    "y2_f = partial(f, \"y2\")\n",
    "y1.register_hook(y1_f)\n",
    "y2.register_hook(y2_f)\n",
    "\n",
    "loss.backward()\n",
    "print(\"x_grad:\", x.grad)"
   ]
  },
  {
   "source": [
    "loss.backward()语句调用之后, 以此发生以下计算过程：\n",
    "\n",
    "1. loss(是一个scalar)把自己的grad梯度赋值为1, 也就是loss在计算图中的upstream gradient\n",
    "\n",
    "2. loss根据自身梯度以及关联的backward方法, 计算出其对应的自变量即y1和y2的梯度, 将该梯度赋值到y1.grad和y2.grad(后续会被删除)\n",
    "\n",
    "3. y2和y1根据自身梯度以及关联的backward方法, 分别计算出对应的自变量x的梯度, x.grad将其收到的多个梯度值进行累加。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**正是因为求导链式法则衍生的梯度累加规则, 张量的grad梯度不会自动清零, 在需要的时候需要手动设置零**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.)\ntensor(0.)\n"
    }
   ],
   "source": [
    "# 下面演示梯度清0的操作\n",
    "from functools import partial\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "for i in range(2):\n",
    "    loss.backward(retain_graph=True)\n",
    "    # 1.如果不清0, 会输出4, 8\n",
    "    # 2.直接清0: x.grad.zero_()\n",
    "    # 3.使用detach\n",
    "    grad = x.grad.detach_()\n",
    "    grad.zero_()\n",
    "    print(x.grad)"
   ]
  },
  {
   "source": [
    "## 四、叶子节点和非叶子节点\n",
    "执行下面代码, 我们会发现loss.grad并不是我们期望的1, 而是None。\n",
    "\n",
    "类似地y1.grad和y2.grad也是None。\n",
    "\n",
    "这是由于它们不是叶子节点张量。\n",
    "\n",
    "在反向传播过程中, 只有is_leaf=True的叶子节点, 且需要求导的张量的导数结果才会被最后保留下里。\n",
    "\n",
    "叶子节点张量一般需要满足两个条件：\n",
    "\n",
    "1. 一般requires_grad为False就默认为叶子节点张量(requires_grad为False意味着在这次backward中并没有什么需要计算的依赖)。\n",
    "\n",
    "2. 由用户创建的requires_grad为True的张量也是叶子节点丈量。\n",
    "\n",
    "Pytorch这样设计的主要目的是为了节省内存或者显存空间。\n",
    "\n",
    "如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss.grad: None\ny1.grad: None\ny2.grad None\ntensor(4.)\n"
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)  # 用户创建的requires_grad为True的叶子节点\n",
    "\n",
    "y1 = x + 1  # 不是叶子节点, 但是由于依赖x, 所以requires_grad自动被设置为True\n",
    "\n",
    "y2 = 2*x  # 和y1一样\n",
    "\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y1.grad:\", y1.grad)\n",
    "print(\"y2.grad\", y2.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x: True True\ny1: False True\ny2: False True\nloss: False True\n"
    }
   ],
   "source": [
    "print(\"x:\", x.is_leaf, x.requires_grad)\n",
    "print(\"y1:\", y1.is_leaf, y1.requires_grad)\n",
    "print(\"y2:\", y2.is_leaf, y2.requires_grad)\n",
    "print(\"loss:\", loss.is_leaf, loss.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss.grad: tensor(1.)\ntensor(4.)\n"
    }
   ],
   "source": [
    "# 使用retain_grad保留非叶子节点的梯度\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "# 保留loss的grad\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "## 五、计算图在TensorBoard中可视化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x@self.w + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "net = Net()\n",
    "logdir = \"./data/tensorboard/2_3_autograph\"\n",
    "writer = SummaryWriter(logdir)\n",
    "writer.add_graph(net, input_to_model=torch.rand(10, 2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No known TensorBoard instances running.\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-4a251fb023868cc7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-4a251fb023868cc7\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "#在tensorboard中查看模型\n",
    "notebook.start(\"--logdir \" + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}