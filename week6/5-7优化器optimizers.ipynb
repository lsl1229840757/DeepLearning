{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-7 优化器optimizers\n",
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。\n",
    "\n",
    "此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一 优化器的使用\n",
    "优化器主要是用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代, 或者直接使用minimizer方法对目标函数进行迭代优化。\n",
    "\n",
    "初始化优化器的时候会创建一个optimizer.iterarions用于记录迭代的次数。因此优化器和tf.Variable一样, 一般需要在@tf.function之外创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求 f(x) = a*x**2 + b*x + c的最小值\n",
    "x = tf.Variable(0.0, name = \"x\", dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = a*x**2 + b*x + c\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    optimizer.apply_gradients(zip([dy_dx], [x]))\n",
    "\n",
    "    if tf.math.mod(optimizer.iterations, 100) == 0:\n",
    "        tf.print(\"iteration :\", optimizer.iterations)\n",
    "        tf.print(\"x = \", x)\n",
    "    return a*x**2 + b*x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100\n",
      "x =  0.867380381\n",
      "iteration : 200\n",
      "x =  0.98241204\n",
      "iteration : 300\n",
      "x =  0.997667611\n",
      "iteration : 400\n",
      "x =  0.999690711\n",
      "iteration : 500\n",
      "x =  0.999959\n",
      "iteration : 600\n",
      "x =  0.999994516\n",
      "iteration : 700\n",
      "x =  0.999998569\n",
      "iteration : 800\n",
      "x =  0.999998569\n",
      "iteration : 900\n",
      "x =  0.999998569\n",
      "iteration : 1000\n",
      "x =  0.999998569\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    minimizef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二 内置优化器\n",
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "在keras.optimizers子模块中，它们基本上都有对应的类的实现。\n",
    "\n",
    "* SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Accelerated Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。\n",
    "\n",
    "* Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。\n",
    "\n",
    "* RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。\n",
    "\n",
    "* Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。\n",
    "\n",
    "* Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了一阶动量。\n",
    "\n",
    "* Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
